{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sparsembed import model\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "import sparsembed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opzetten van de functies\n",
    "\n",
    "#### 1. cosine_similarity(a, b): \n",
    "Deze functie berekent de cosine similarity tussen twee vectoren. Cosine similarity is een maatstaf die aangeeft hoe vergelijkbaar twee vectoren zijn, ongeacht hun grootte.\n",
    "\n",
    "#### 2. embed(text_to_embed):\n",
    "Deze functie zet een gegeven tekst om in sparse embeddings met behulp van het Splade-model. Embeddings zijn vectorrepresentaties van tekst die de betekenis van de tekst in een numerieke vorm vastleggen.\n",
    "\n",
    "#### 3. create_vector_db(file_path, output_path='vector_db.json'):\n",
    "Deze functie maakt een vector-database van een tekstbestand en slaat deze op in een JSON-bestand. Het leest de zinnen uit een bestand, genereert embeddings voor elke zin, en slaat deze embeddings samen met de originele zinnen op in een JSON-bestand.\n",
    "\n",
    "#### 4. load_vector_data(file_path):\n",
    "Deze functie laadt vectorgegevens uit een JSON-bestand en zet ze terug om naar tensors. Dit stelt ons in staat om de opgeslagen vectorrepresentaties opnieuw te gebruiken.\n",
    "\n",
    "#### 5. get_splade_embeddings(example_question):\n",
    "Deze functie zet een gegeven vraag om in sparse embeddings met behulp van het Splade-model. Dit is vergelijkbaar met de embed functie, maar specifiek voor het verwerken van vragen.\n",
    "\n",
    "#### 6. find_most_similar_sentence(example_question, loaded_tensors, loaded_sentences):\n",
    "Deze functie vindt de meest vergelijkbare zin ten opzichte van een gegeven vraag uit een lijst van vooraf geladen zinnen. Door cosine similarity te berekenen tussen de vector van de vraag en de vectoren van de geladen zinnen, kunnen we de meest vergelijkbare zin identificeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a: np.ndarray, b: np.ndarray) -> np.float64:\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between two vectors.\n",
    "\n",
    "    Args:\n",
    "        a (np.ndarray): First input vector.\n",
    "        b (np.ndarray): Second input vector.\n",
    "\n",
    "    Returns:\n",
    "        np.float64: Cosine similarity between vector a and vector b.\n",
    "    \"\"\"\n",
    "    dot = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    return dot / ( norm_a * norm_b )\n",
    "    \n",
    "cos_sim = np.vectorize(cosine_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(text_to_embed: str) -> dict:\n",
    "    \n",
    "    \"\"\"\n",
    "    Encodes a given text into sparse embeddings using the Splade model.\n",
    "\n",
    "    Args:\n",
    "        text_to_embed (str): The text to be encoded into embeddings.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the sparse activations from the embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    splade_model = sparsembed.model.Splade(\n",
    "    model=AutoModelForMaskedLM.from_pretrained(\"naver/splade_v2_max\"),\n",
    "    tokenizer=AutoTokenizer.from_pretrained(\"naver/splade_v2_max\")\n",
    "    )\n",
    "\n",
    "# Encode text into embeddings using Splade\n",
    "    with torch.no_grad():\n",
    "        # Encode the input text into embeddings\n",
    "        embeddings = splade_model.encode(\n",
    "            texts=[text_to_embed],  # Provide the input text as a list\n",
    "            truncation=\"longest_first\"  # Choose a valid truncation strategy if needed\n",
    "        )\n",
    "\n",
    "# Display embeddings\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_db(file_path: str, output_path: str = 'vector_db.json') -> None:\n",
    "    \"\"\"\n",
    "    Creates a vector database from a text file and saves it to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the text file containing the sentences.\n",
    "        output_path (str, optional): The path where the JSON file will be saved. Defaults to 'vector_db.json'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # List to store lines from the text file\n",
    "    lines = []\n",
    "\n",
    "    # Read the text file and process each line\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            # Strip leading and trailing whitespace, including newlines\n",
    "            line = line.strip()\n",
    "            # Skip empty lines\n",
    "            if line:\n",
    "                # Append the non-empty line to the list\n",
    "                lines.append(line)\n",
    "\n",
    "    # Generate embeddings for the lines\n",
    "    embeddinglist = list(map(embed, lines))\n",
    "\n",
    "    # Extract sparse activations from the embeddings\n",
    "    tensor_list = [d['sparse_activations'] for d in embeddinglist if 'sparse_activations' in d]\n",
    "\n",
    "    # Convert tensors to lists so they can be saved in JSON\n",
    "    lists = [tensor.tolist() for tensor in tensor_list]\n",
    "\n",
    "    # Create a dictionary to store arrays and sentences\n",
    "    data = {\"arrays\": lists, \"sentences\": lines}\n",
    "\n",
    "    # Save data to a JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data, f)\n",
    "\n",
    "    print(f\"Vector database saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vector_data(file_path: str) -> tuple[list[torch.tensor], list[str]]:\n",
    "\n",
    "    \"\"\"\n",
    "    Loads vector data from a JSON file and converts it back to tensors.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the JSON file containing the vector data.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing two elements:\n",
    "            - loaded_tensors (list of torch.Tensor): A list of tensors converted from the stored lists.\n",
    "            - loaded_sentences (list of str): A list of sentences corresponding to the vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "        loaded_data = json.load(f)\n",
    "    \n",
    "    # Retrieve lists and sentences\n",
    "    loaded_lists = loaded_data[\"arrays\"]\n",
    "    loaded_sentences = loaded_data[\"sentences\"]\n",
    "    \n",
    "    # Convert lists back to tensors\n",
    "    loaded_tensors = [torch.tensor(lst) for lst in loaded_lists]\n",
    "    \n",
    "    return loaded_tensors, loaded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splade_embeddings(example_question: str) -> torch.tensor:\n",
    "\n",
    "    \"\"\"\n",
    "    Encodes a given question into sparse embeddings using the Splade model.\n",
    "\n",
    "    Args:\n",
    "        example_question (str): The question to be encoded into embeddings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The sparse activations from the embeddings.\n",
    "    \"\"\"\n",
    "    \n",
    "    splade_model = sparsembed.model.Splade(\n",
    "        model=AutoModelForMaskedLM.from_pretrained(\"naver/splade_v2_max\"),\n",
    "        tokenizer=AutoTokenizer.from_pretrained(\"naver/splade_v2_max\")\n",
    "    )\n",
    "\n",
    "    # Encode text into embeddings using Splade\n",
    "    with torch.no_grad():\n",
    "        # Encode the input text into embeddings\n",
    "        embeddings = splade_model.encode(\n",
    "            texts=[example_question],  # Provide the input text as a list\n",
    "            truncation=\"longest_first\"  # Choose a valid truncation strategy if needed\n",
    "        )\n",
    "\n",
    "    return embeddings[\"sparse_activations\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar_sentence(example_question: str, loaded_tensors: list[torch.tensor], loaded_sentences: list[str]) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    Finds the most similar sentence to the given question from a list of pre-loaded sentences.\n",
    "\n",
    "    Args:\n",
    "        example_question (str): The question to compare against the loaded sentences.\n",
    "        loaded_tensors (list of torch.Tensor): The list of tensors representing the embeddings of the loaded sentences.\n",
    "        loaded_sentences (list of str): The list of pre-loaded sentences corresponding to the embeddings.\n",
    "\n",
    "    Returns:\n",
    "        str: The sentence from the loaded sentences that is most similar to the given question.\n",
    "    \"\"\"\n",
    "    \n",
    "    sparse_embedding_input = get_splade_embeddings(example_question)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = [cosine_similarity(i, sparse_embedding_input.T)[0, 0] for i in loaded_tensors]\n",
    "    \n",
    "    # Find the index of the max similarity value\n",
    "    max_value_index = similarities.index(max(similarities))\n",
    "    \n",
    "    # Return the most similar sentence\n",
    "    return loaded_sentences[max_value_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gebruik van de gemaakte functies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Om de functionaliteit van de hierboven beschreven functies te demonstreren, zullen we een aantal taken uitvoeren.\n",
    "Eerst maken we een vectordatabase json bestand. vervolgens gaan we de gemaakte functies gebruiken om de meest relevante zin op te halen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector database saved to vector_db.json\n"
     ]
    }
   ],
   "source": [
    "# Creates the json file for the with the sentences and embeddings\n",
    "create_vector_db(r\"parsed.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article 3 | None | 33 | ‘biometric data’ means personal data resulting from specific technical processingrelating to the physical, physiological or behavioural characteristics of a natural person, such as facial images or dactyloscopic data;\n"
     ]
    }
   ],
   "source": [
    "# Example usage of the functions created above\n",
    "example_question = \"How do I process biometric data and other personal data responsibly with AI for my company?\"\n",
    "file_path = 'vector_db.json'\n",
    "loaded_tensors, loaded_sentences = load_vector_data(file_path) # deze duurt lang dus die miss ergens anders aanroepen en saven (dit is de database inladen)\n",
    "\n",
    "result = find_most_similar_sentence(example_question, loaded_tensors, loaded_sentences)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top N similarities\n",
    "\n",
    "In dit hoofdstuk breiden we onze analyse uit door de mogelijkheid toe te voegen om de top N meest vergelijkbare zinnen op te halen voor een gegeven vraag. We beginnen met het laden van de vooraf geladen vectorgegevens en zinnen uit een JSON-bestand, zoals eerder beschreven. Vervolgens implementeren we een functie die gebruikmaakt van cosine similarity om de zinnen te rangschikken en de top N resultaten te selecteren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'vector_db.json'\n",
    "loaded_tensors, loaded_sentences = load_vector_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3) -> pd.DataFrame:\n",
    "\n",
    "    \"\"\"\n",
    "    Finds the top N most similar sentences to the given question from a list of pre-loaded sentences.\n",
    "\n",
    "    Args:\n",
    "        example_question (str): The question to compare against the loaded sentences.\n",
    "        loaded_tensors (list of torch.Tensor): The list of tensors representing the embeddings of the loaded sentences.\n",
    "        loaded_sentences (list of str): The list of pre-loaded sentences corresponding to the embeddings.\n",
    "        top_n (int, optional): The number of top similar sentences to return. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the top N most similar sentences, their tensors, and their similarity scores.\n",
    "    \"\"\"\n",
    "    sparse_embedding_input = get_splade_embeddings(example_question)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = [cosine_similarity(i.unsqueeze(0), sparse_embedding_input.T)[0, 0] for i in loaded_tensors]\n",
    "    \n",
    "    # Create a DataFrame with sentences, tensors, and similarities\n",
    "    df = pd.DataFrame({\n",
    "        'sentences': loaded_sentences,\n",
    "        'tensors': loaded_tensors,\n",
    "        'similarity': similarities\n",
    "    })\n",
    "\n",
    "    # Sort by similarity and get the top N entries\n",
    "    top_n_df = df.sort_values(by='similarity', ascending=False).head(top_n)\n",
    "    \n",
    "    return top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Article 3 | None | 33 | ‘biometric data’ means...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.40864322]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>Article 7 | None | ba | the nature and amount ...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.3528345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Article 3 | None | 35 | ‘biometric categorisat...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.33119974]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "245  Article 3 | None | 33 | ‘biometric data’ means...   \n",
       "304  Article 7 | None | ba | the nature and amount ...   \n",
       "251  Article 3 | None | 35 | ‘biometric categorisat...   \n",
       "\n",
       "                                               tensors    similarity  \n",
       "245  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.40864322]  \n",
       "304  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   [0.3528345]  \n",
       "251  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.33119974]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage of the function created above\n",
    "example_question = \"How do I process biometric data and other personal data responsibly with AI for my company?\"\n",
    "file_path = 'vector_db.json'\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluatie van Robuustheid en Performantie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In dit hoofdstuk voeren we verschillende testcases uit om de robuustheid en performantie van ons systeem te evalueren. We richten ons specifiek op het gedrag van de functie die de meest vergelijkbare zinnen identificeert op basis van een gegeven vraag. Hierbij worden de volgende testcases behandeld:\n",
    "\n",
    "Ongerelateerde Vraag: We testen hoe ons systeem omgaat met een vraag die weinig of geen overeenkomst heeft met de opgeslagen zinnen. Dit helpt ons te begrijpen of het systeem in staat is om onderscheid te maken tussen relevante en niet-relevante input.\n",
    "\n",
    "Ongerelateerd Woord als Input: We onderzoeken hoe ons systeem reageert wanneer een enkelvoudig, ongerelateerd woord als input wordt gegeven in plaats van een volledige vraag. Dit testgeval helpt ons de capaciteit van het systeem te beoordelen om met onvolledige of onduidelijke input om te gaan.\n",
    "\n",
    "Reeks Gerelateerde Woorden als Input: We stellen een reeks gerelateerde woorden als input voor. Dit scenario simuleert een situatie waarin de vraag complexer is en meerdere aspecten of contextuele informatie bevat. We evalueren of ons systeem in staat is om de juiste zinnen te identificeren die overeenkomen met de gegeven context.\n",
    "\n",
    "Door verschillende types van input te testen, kunnen we beoordelen of ons systeem consistent blijft in het identificeren van relevante zinnen en het negeren van irrelevante informatie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testcase 1: Ongerelateerde vragen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>Article 3 | None | 44c | ‘non-personal data’ m...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.06366091]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>III. CONCLUSION | ANNEX | 47 | To address conc...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.041683216]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>III. CONCLUSION | ANNEX | 69 | In order to fac...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.037700266]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "267  Article 3 | None | 44c | ‘non-personal data’ m...   \n",
       "90   III. CONCLUSION | ANNEX | 47 | To address conc...   \n",
       "148  III. CONCLUSION | ANNEX | 69 | In order to fac...   \n",
       "\n",
       "                                               tensors     similarity  \n",
       "267  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   [0.06366091]  \n",
       "90   [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.041683216]  \n",
       "148  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.037700266]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question = \"Hi, how are you?\"\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>III. CONCLUSION | ANNEX | 70a | A variety of A...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.06646946]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>article 52a | None | d | input and output moda...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.060234632]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Article 58b | None | iiii | providing advice o...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.05739121]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "150  III. CONCLUSION | ANNEX | 70a | A variety of A...   \n",
       "764  article 52a | None | d | input and output moda...   \n",
       "578  Article 58b | None | iiii | providing advice o...   \n",
       "\n",
       "                                               tensors     similarity  \n",
       "150  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   [0.06646946]  \n",
       "764  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.060234632]  \n",
       "578  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   [0.05739121]  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tweede ongerelateerde vraag\n",
    "example_question = \"What different type of horses are there\"\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testcase2: een ongerelateerd woord als input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Article 3 | None | 14 | ‘safety component of a...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.08789998]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Article 3 | None | 1a | ‘risk’ means the combi...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.08459283]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>Article 3 | None | bk | ‘informed consent’ mea...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.07527994]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "226  Article 3 | None | 14 | ‘safety component of a...   \n",
       "214  Article 3 | None | 1a | ‘risk’ means the combi...   \n",
       "274  Article 3 | None | bk | ‘informed consent’ mea...   \n",
       "\n",
       "                                               tensors    similarity  \n",
       "226  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.08789998]  \n",
       "214  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.08459283]  \n",
       "274  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.07527994]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question = \"Football\"\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Article 3 | None | 15 | ‘instructions for use’...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.09187493]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Article 3 | None | 2 | ‘provider’ means a natu...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.09030487]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Article 3 | None | 1 | ‘AI system‘ is a machin...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.08032041]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "227  Article 3 | None | 15 | ‘instructions for use’...   \n",
       "215  Article 3 | None | 2 | ‘provider’ means a natu...   \n",
       "213  Article 3 | None | 1 | ‘AI system‘ is a machin...   \n",
       "\n",
       "                                               tensors    similarity  \n",
       "227  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.09187493]  \n",
       "215  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.09030487]  \n",
       "213  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.08032041]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question = \"apple pie\"\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testcase 3: reeks gerelateerde woorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Article 3 | None | 33 | ‘biometric data’ means...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.41524762]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>Article 3 | None | 35 | ‘biometric categorisat...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.3791988]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Article 3 | None | 33c | ‘biometric verificati...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.37913036]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "245  Article 3 | None | 33 | ‘biometric data’ means...   \n",
       "251  Article 3 | None | 35 | ‘biometric categorisat...   \n",
       "247  Article 3 | None | 33c | ‘biometric verificati...   \n",
       "\n",
       "                                               tensors    similarity  \n",
       "245  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.41524762]  \n",
       "251  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   [0.3791988]  \n",
       "247  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.37913036]  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question = \"Biometric, Biometric, Biometric, Biometric\"\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>tensors</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Article 3 | None | 1a | ‘risk’ means the combi...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.53656644]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>462</th>\n",
       "      <td>Article 52a | None | None | risk if it meets a...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.3569135]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>Article 6 | None | None | independently from t...</td>\n",
       "      <td>[[tensor(0.), tensor(0.), tensor(0.), tensor(0...</td>\n",
       "      <td>[0.32530776]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentences  \\\n",
       "214  Article 3 | None | 1a | ‘risk’ means the combi...   \n",
       "462  Article 52a | None | None | risk if it meets a...   \n",
       "296  Article 6 | None | None | independently from t...   \n",
       "\n",
       "                                               tensors    similarity  \n",
       "214  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.53656644]  \n",
       "462  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...   [0.3569135]  \n",
       "296  [[tensor(0.), tensor(0.), tensor(0.), tensor(0...  [0.32530776]  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_question = \"risk, risk, risk, risk\"\n",
    "top_n_df = find_top_n_similar_sentences(example_question, loaded_tensors, loaded_sentences, top_n=3)\n",
    "\n",
    "# Display the result\n",
    "top_n_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie en advies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusie\n",
    "Na het uitvoeren van de testcases om de functionaliteit van het systeem te evalueren, kunnen we de volgende conclusies trekken:\n",
    "\n",
    "#### Testcase 1: Ongerelateerde vragen\n",
    "Voorbeeld: De input was gericht op niet-persoonlijke gegevens, zoals beschreven in juridische artikelen.\n",
    "Resultaat: Het systeem toonde een lage similarity score voor alle zinnen, wat aangeeft dat het goed onderscheid maakt tussen gerelateerde en ongerelateerde vragen.\n",
    "\n",
    "#### Testcase 2: Een ongerelateerd woord als input\n",
    "Voorbeeld: Een enkelvoudig, niet-gerelateerd woord werd gebruikt als input, zoals \"Football\".\n",
    "Resultaat: Het systeem produceerde zinnen met lage similarity scores.\n",
    "\n",
    "#### Testcase 3: Reeks gerelateerde woorden\n",
    "Voorbeeld: Een reeks gerelateerde woorden, zoals \"biometric data\", werd gebruikt als input.\n",
    "Resultaat: Het systeem identificeerde zinnen met hogere similarity scores die specifiek gerelateerd waren aan het onderwerp \"biometric data\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advies en verbetering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Uitbreiden van de Testsample: \n",
    "Om een meer robuuste evaluatie uit te voeren, adviseren we het gebruik van een grotere testsample. Dit zal helpen bij het vaststellen van een geschikte drempelwaarde voor de similarity scores.\n",
    "\n",
    "#### 2. Toevoegen van een threshold: \n",
    "Na het uitbreiden van de testsample is het een optie om een threshhold toe te voegen zodat RAG alleen tekst ophaalt die relevant genoeg is. Dit kan worden bereikt door een statistische analyse van de scores uit te voeren om een beter begrip te krijgen van wat als \"relevant\" moet worden beschouwd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interesting sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://milvus.io/docs/embed-with-splade.md\n",
    "2. https://colab.research.google.com/github/pinecone-io/examples/blob/master/learn/search/semantic-search/sparse/splade/splade-vector-generation.ipynb\n",
    "3. https://zilliz.com/learn/discover-splade-revolutionize-sparse-data-processing\n",
    "4. https://medium.com/@saschametzger/what-are-tokens-vectors-and-embeddings-how-do-you-create-them-e2a3e698e037\n",
    "5. https://www.rungalileo.io/blog/mastering-rag-how-to-select-an-embedding-model\n",
    "6. https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "7. https://www.youtube.com/watch?v=wvk5uxMwMYs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
